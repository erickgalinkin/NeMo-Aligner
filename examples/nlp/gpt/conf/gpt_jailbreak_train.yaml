trainer:
  devices: 1
  num_nodes: 1
  accelerator: gpu
  precision: bf16 # 16, 32, or bf16

  deep_search:
    max_steps: -1
    val_check_interval: 1
    max_epochs: 1
    save_interval: ${.val_check_interval}

    gradient_clip_val: 1.0
    limit_val_batches: 0.1

    # how many batches per training step
    num_value_batches: 2
    num_policy_batches: 1

  # no need to change these
  logger: False # logger provided by exp_manager
  enable_checkpointing: False
  use_distributed_sampler: False
  max_time: null
  max_epochs: ${.deep_search.max_epochs}
  max_steps: ${.deep_search.max_steps}

pretrained_checkpoint:
  restore_from_path: null
  from_mcts_trained: False
  has_value_head: True  # whether the pretrained model has value head

mcts_data_file: null

server_url: ??  # the rabbitmq server url, e.g. pyamqp://guest@localhost:5672//
backend_url: rpc://  # the backend server url, e.g. rpc://

dataset:
  name: jailbreak
  prompt_template_name: ''

model:
  from_mcts_trained: ${pretrained_checkpoint.from_mcts_trained}
  mcts:
    C: 2 # weight for the UCB prior term
    num_searches: 200  # number of MCTS searches

    rollout_micro_batch_size: 1 # batch size for each dp worker to handle
    num_rollouts: 1.0 # float(will be % of the dataset) or int

    temperature: 0.2  # use low temperature for more greedy search
    dirichlet_epsilon: 0.0  # weight for dirichlet noise added to the root state, turn off the dirichlet noise by setting this to 0
    dirichlet_alpha: 0.3 # parameter for dirichlet noise, the prior probability of the action happens
    max_depth: 500  # maximum depth of the search tree

    save_timer: 13500 # save the intermediate search results at 3 hours and 45 minutes
    cache_dir: null  # cache dir for the search results

    top_k: 50
    end_strings: ["<extra_id_1>", "\x11"]  # generation will stop when one of these tokens is generated
    add_bos_token: False # add the bos token at the beginning of the prompt

    oracle: True  # whether to use the oracle value for the search results, Default True, so it uses ground truth value when it reaches the terminal state
    turn_off_value: ${not:${pretrained_checkpoint.from_mcts_trained}}

    kv_cache_in_cpu: True #  save kv cache in cpu memory

    environment: null  # the environment for MCTS to interact with, default None, can be set to "code" for running code
    feedback: jailbreak # the feedback function used for the search, ['math', 'steerlm']

    turn_off_kv_cache: False # turn off the kv cache for inference

    train:
      value_weight: 1 # weight of the value portion of the loss
      policy_weight: 1 # weight of the policy portion of the loss

  inference:

    micro_batch_size: 16

    sampling_params:
      use_greedy: True # Whether to use sampling ; use greedy decoding otherwise
      top_k: ${model.mcts.top_k}  # The number of highest probability vocabulary tokens to keep for top-k-filtering.
      top_p: 0.9 # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.
      temperature: 1.0 # sampling temperature
      repetition_penalty: 1.2  # The parameter for repetition penalty. 1.0 means no penalty.
      add_BOS: False # add the bos token at the begining of the prompt
      all_probs: False  # whether return the log prob for all the tokens in vocab
      compute_logprob: False  # a flag used to compute logprob of all the input text, a very special case of running inference, default False
      end_strings: ${model.mcts.end_strings}

    length_params:
      max_length: ${int_div:${model.encoder_seq_length}, 2}
      min_length: 1

  offload_adam_states: False

  micro_batch_size: 1
  global_batch_size: 8

  # each sample in the critic is a lot
  critic_global_batch_size: ${.global_batch_size}

  mcore_gpt: True
  share_embeddings_and_output_weights: False

  # reward_model_type: binary_ranking # ["binary_ranking, "regression"]
  regression:
    num_attributes: 1 # dimension of regression head
    merge_attributes: False # whether to merge multiple attributes into a scalar
    attribute_weights: null # apply these weights to each attributes when merging them into a scalar
    loss_mask_val: -100 #  mask dimensions with this value when calculating MSE loss

  output_sequence: True  # Whether to output a single scalar or a sequence of scalars.
  use_avg_pool: False  # Whether to use avg pool to sum across the sequence dim in reward model
  force_head_dtype: float32  # enforce specific dtype for the final projection in the model head
  megatron_amp_O2: True
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  pipeline_model_parallel_split_rank: 0 # used for encoder and decoder model (0 for others)
  encoder_seq_length: 4096
  max_position_embeddings: ${model.encoder_seq_length}

  # parameters for value output
  value:
    max_position_embeddings: ${model.encoder_seq_length}
    seed: 1234
    num_layers: 2 # two layers
    tensor_model_parallel_size: ${model.tensor_model_parallel_size}
    megatron_amp_O2: False

    optim:
      name: distributed_fused_adam
      bucket_cap_mb: 200
      overlap_grad_sync: False
      contiguous_grad_buffer: True
      lr: 1e-6
      weight_decay: 0.01
      betas:
      - 0.9
      - 0.98
      sched:
        name: CosineAnnealing
        warmup_steps: 5
        constant_steps: 10
        min_lr: 1e-7

    # define fields from the base model's config that should be ignored when merging with this config.
    overwrite_base_config:
      data:
        data_prefix: True
      optim: True

  # miscellaneous
  seed: 1234

  optim:
    name: distributed_fused_adam
    bucket_cap_mb: 200
    overlap_grad_sync: False
    contiguous_grad_buffer: True
    lr: 9e-7
    weight_decay: 0.01
    betas:
    - 0.9
    - 0.98
    sched:
      name: CosineAnnealing
      warmup_steps: 5
      constant_steps: 10
      min_lr: 1e-7

  precision: ${trainer.precision}

  # define fields from the base model's config that should be ignored when merging with this config.
  overwrite_base_config:
    data:
      data_prefix: True
    optim: True

exp_manager:
  explicit_log_dir: /results
  exp_dir: null
  name: jailbreak_megatron_gpt_ppo_hybrid
  create_wandb_logger: False
  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  create_checkpoint_callback: False
  checkpoint_callback_params:
    monitor: val_global_accuracy
    save_top_k: 3
    mode: max
    always_save_nemo: False # saves nemo file during validation, not implemented for model parallel
    save_nemo_on_train_end: True # not recommended when training large models on clusters with short time limits
    filename: 'jailbreak_megatron_gpt-{step}-{consumed_samples}-{consumed_samples_values}-{policy_optimization_step}-{value_optimization_step}-{epoch}-{val_global_accuracy:.3f}'
    model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}
